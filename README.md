Apache Spark is a very powerful tool used for Big data. It has been used by companies and universities. In this small tutorial/article I show similarities/differences between the ML library of PySpark vs Scikit-Learn, a conventional python library for machine learning. I follow the same steps in both cases. 

Before applying any regression or classification method, I will have to discover my dataset...as it always has to be. The steps that I will follow will be:
- Data Cleaning
- Exploratory Data Analysis (EDA)
- Data Preprocessing
- A Regression instance
- A Classification instance


**Working Scenario**:
A data scientist of a telecommunications company has been asked to predict the monthly & total cost for the services that a client has. The essential, however, would be to forecast the client who is going to resigns the contract.


This dataset can be found in kaggle(https://www.kaggle.com/datasets/blastchar/telco-customer-churn). IBM has also a similar dataset (https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=samples-telco-customer-churn).

<img width="1001" alt="Churn_preprocessing_pyspark" src="https://user-images.githubusercontent.com/86191637/227220609-6ad50c15-18e8-48f3-b8a4-726ba4bd3a2d.png">
