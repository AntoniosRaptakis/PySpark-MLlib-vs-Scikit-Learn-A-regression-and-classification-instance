Apache Spark is a very powerful tool used for Big data. It has been used by companies and universities in order to manipulate big data and build a wareghouse. The role of this small tutorial/article is to show similarities/differences between the ML library of PySpark vs Scikit-Learn, a conventional python library for machine learning, from the documentation to the results. 

There are two jupyter notebooks, one for pyspark and another one for scikit-learn. I follow the same steps in both cases. These steps are: 

- Data Cleaning
- Data Preprocessing
- A Regression instance
- A Classification instance

In the scikit-learn notebook there is an EDA part.


**Working Scenario**:
A data scientist of a telecommunications company has been asked to predict the monthly & total cost for the services that a client has. The essential, however, would be to forecast the client who is going to resigns the contract.


This dataset can be found in kaggle(https://www.kaggle.com/datasets/blastchar/telco-customer-churn). IBM has also a similar dataset (https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=samples-telco-customer-churn).

<img width="1001" alt="Churn_preprocessing_pyspark" src="https://user-images.githubusercontent.com/86191637/227220609-6ad50c15-18e8-48f3-b8a4-726ba4bd3a2d.png">
